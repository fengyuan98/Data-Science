### K-means

- 无监督

#### 算法步骤

- 选择k个样本作为初始的聚类中心点
- 对于数据集中的每个样本点，计算它到k个聚类中心的距离，并将该样本点归类到距离最小的类中
- 得到份好的类别后，计算每个类别中的聚类中心 $\frac{1}{|c_i|}\sum_{x\in c_i}x$ （该类中所有样本的质心（均值））作为新的聚类中心
- 重复2，3步操作，直至达到某个终止条件（迭代次数，最小误差）

#### 复杂度

- 时间复杂度：O(tknm)，t为迭代次数，k为簇的数目，n为样本点数，m为样本点维度
- 空间复杂度：O(m(n+k))，k为簇的数目，m为样本点维度，n为样本点数

#### 优缺点

- 优点
  - 容易理解，聚类效果不错，虽是局部最优解
  - 处理大数据集时，可保证较好的伸缩性
  - 当簇近似高斯分布时，效果不错
  - 算法复杂度低
- 缺点
  - k需要人为设定，k不同得到的结果也不同
  - 对初始化聚类中心敏感
  - 对异常值敏感
  - 样本只能归为一类，不适合多分类任务
  - 不适合太离散的分类，样本类别不平衡的分类，非凸形状的分类

#### 调优与改进

- 数据预处理：数据归一化，数据标准化，异常点检测
- 合理选择k值：手肘法（人工找拐点），Gap statistic（取最大值时对应的k为最佳）
- 核函数（kernel）

 

### KNN

- 有监督

​	对新输入的待分类数据点，找到与其最相邻的k个已标记数据点，将待分类数据点归为这k个数据点所在类别出现频率最多的类中。

- K过小$\rightarrow$ 模型过于复杂$\rightarrow$ 过拟合（通常用交叉验证选取最优k值）
- Distance Metric

$$
\boldsymbol{x}_i=(x_i^{(1)},\cdots,x_i^{(n)})^T;x_j=(x_j^{(1)},\cdots,x_j^{(n)})^T;\\
L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1/p}\\
when\ \ p=2,\ Euclidean\ \ Distance\\
when\ \ p=1,\ Manhattan\ \ Distance\\
when\ \ p=\infty,\ L_\infty(x_i,x_j)=max|x_i^{(l)}-x_j^{(l)}|
$$

- 归一化

$$
\mathbf{M}_j=max\ x_{ij}-min\ x_{ij}\\
d(y_1,\cdots,y_n)(z_1,\cdots,z_n)=\sqrt{\sum_{j=1}^n(\frac{y_j}{M_j}-\frac{z_j}{M_j})^2}
$$

#### 算法步骤

- 计算新样本与已标记数据集中所有样本的距离

- 按照距离大小进行递增排序
- 选取距离最小的k个样本
- 确定前k个样本所在类别出现的频率，并输出出现频率最高的类别

#### 特点

- 属于惰性学习（lazy-learning）：没有显式的学习过程/训练阶段
- 计算复杂度高：O(n)，所以一般适用于样本数较少的数据集
- K的取值不同，分类结果可能会有显著差异
















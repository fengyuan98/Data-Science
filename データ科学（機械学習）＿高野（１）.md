## 1-1 概率统计复习，贝叶斯定理

- 概率密度函数

- 联合概率密度函数

- 边际分布（Marginalization）

- 条件概率：若$x_1$和$x_2$相互独立，则$p(x_1|x_2,x_3)=p(x_1,|x_3)$

- 贝叶斯定理：$P(x_1|x_2)=\frac{P(x_2|x_1)P(x_1)}{P(x_2)}=\frac{P(x_2|x_1)P(x_1)}{\int P(x_1,x_2)dx_1}=\frac{P(x_2|x_1)P(x_1)}{\int P(x_2|x_1)P(x_1)dx_1}$

  Practice1: show that $P(x_1|x_2,x_3)=P(x_1|x_2,x_3)P(x_2|x_3)$

   Proof: 
  $$
  \begin{align}
  left-hand \ \ side &=\frac{P(x_1,x_2,x_3)}{P(x_3)} \\
  &= \frac{P(x_1|x_2,x_3)P(x_2,x_3)}{P(x_3)} \\
  &= \frac{P(x_1|x_2,x_3)P(x_2|x_3)P(x_3)}{P(x_3)} \\
  &= P(x_1|x_2,x_3)P(x_2|x_3)P(x_3) \\
  &= right-hand \ \ side
  \end{align}
  $$
  Practice2: show that $p(x_3|x_1,x_2)=\frac{p(x_1｜x_2，x_3)p(x_2|x_3)p(x_3)}{p(x_1,x_2)}$

  Proof:
  $$
  左=\frac{p(x_1,x_2|x_3)p(x_3)}{p(x_1,x_2)}=\frac{p(x_1｜x_2，x_3)p(x_2|x_3)p(x_3)}{p(x_1,x_2)}=右
  $$

- 相关系数：$r=\frac{\sigma_{xy}}{\sigma_x \sigma_y}$
  $$
  \begin{align}
  &\sigma_x^2=\int p(x)x^2\ dx \\
  &\sigma_y^2=\int p(y)y^2\ dx \\
  &\sigma_{xy}=\int p(x,y)xy\ dx\ dy
  \end{align}
  $$
  期望：$\bar{x}=\int p(x)x \ dx=0$；$\bar{y}=\int p(y)y \ dy=0$

## 1-2 正态分布的参数优化

### 正态分布

$$
p(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\{-\frac{(x-\mu)^2}{2\sigma^2} \}
$$

### 多元正态分布

$$
p(\boldsymbol{x}|\boldsymbol{\mu},\Sigma)=\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}exp\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \}
$$

### Optimization Problem:

Maximize the logarithm of the joint probability for data $x_1,x_2,\cdots, x_N$
$$
max_{\boldsymbol{\mu},\Sigma} \ \ \sum_{i=1}^N ln\ p(\boldsymbol{x}_i|\boldsymbol{\mu},\Sigma)
$$
Objective Function:
$$
\begin{align}
\psi(\boldsymbol{\mu},\Sigma)&= \sum_{i=1}^N ln\ p(\boldsymbol{x}_i|\boldsymbol{\mu},\Sigma) \\
&= \sum_{i=1}^N [-\frac{n}{2}ln2\pi + \frac{1}{2}ln|\Sigma^{-1}|-\frac{1}{2}(\boldsymbol{x}_i-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\mu})]
\end{align}
$$

$$
\begin{align}
<1>\ \ \ \ &\frac{\partial \psi(\boldsymbol{\mu},\Sigma)}{\partial \boldsymbol{\mu}}
=\sum_{i=1}^N (\boldsymbol{x}_i-\boldsymbol{\mu})^T\Sigma^{-1}=0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\                 
& \sum_{i=1}^N (\boldsymbol{x}_i-\boldsymbol{\mu})^T=0 \\
& \sum_{i=1}^N \boldsymbol{x}_i = \sum_{i=1}^N \boldsymbol{\mu} \\
& \boldsymbol{\mu} = \sum_{i=1}^N \boldsymbol{x}_i/N
\end{align}
$$

> **<u>附录</u>**
>
> 对于对称矩阵$A \in R^{n \times n}$，它的偏导满足以下关系：
>
> （1）$\frac{\partial ln|A|}{\partial A}=2A^{-1}-diag(A^{-1})$
>
> （2）$\frac{\partial tr(AB)}{\partial A}=\frac{\partial \boldsymbol{x}^TA\boldsymbol{x}}{\partial A} = 2B-diag(B)$   where $B=\boldsymbol{x}\boldsymbol{x}^T$

$$
\begin{align}
<2>\ \ \ \ &\frac{\partial \psi(\boldsymbol{\mu},\Sigma)}{\partial \Sigma^{-1}}
=\sum_{i=1}^N [\frac{1}{2}\{2\Sigma-diag(\Sigma)\}-\frac{1}{2}\{2(\boldsymbol{x}_i-\boldsymbol{\mu})(\boldsymbol{x}_i-\boldsymbol{\mu})^T-diag((\boldsymbol{x}_i-\boldsymbol{\mu})(\boldsymbol{x}_i-\boldsymbol{\mu})^T)\}] \\
& \frac{1}{2}\sum_{i=1}^N \{2\Sigma-2  (\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T\}=\frac{1}{2}\sum_{i=1}^N\{diag(\Sigma-(\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T) \} \\
& Letting \ \ S=\sum_{i=1}^N \{2\Sigma-2  (\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T\} \\
& we \ \ obtain \ \ S=\frac{1}{2}diag(S)
\end{align}
$$

$S$的非对角线上的元素都是0，且对角线上元素也为0，即：
$$
\begin{align}
& \sum_{i=1}^N \{2\Sigma-2  (\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T\}=0  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
& N\Sigma = \sum_{i=1}^N\{(\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T\} \\
& \Sigma = \sum_{i=1}^N \{(\boldsymbol{x}_i-\boldsymbol{\mu}) (\boldsymbol{x}_i-\boldsymbol{\mu})^T\}/N
\end{align}
$$

## 1-3 离散概率分布的参数优化

$p(k)$：离散随机变量的概率分布

$x \in \{1,2,\cdots,n\}$ 

$\theta =\{p(1),p(2),\cdots,p(n)\}$

Optimization Problem：$max_\theta \ \ \prod_{i=1}^N p(x_i|\theta)$ 

等价于：$max_\theta \ \ \sum_{i=1}^Nlnp(x_i|\theta)$   subject to $\sum_{k=1}^n p(k)=1$ 

其拉格朗日形式：$L(\theta)=\sum_{i=1}^Nln \ p(x_i|\theta)-\lambda(\sum_{k=1}^n p(k)=1)$ 
$$
\begin{align}
<1>\ \ \  & \frac{\partial L(\theta)}{\partial p(k)}=\sum_{i=1}^N \frac{\delta(x_i,k)}{p(k)}-\lambda=0 \\
\end{align}
$$
这里的$\delta(x,k)$是$Kronecker \ \ \ delta$，
$$
\delta(x,k)=\begin{cases}
0 & & (x≠k) \\
1 & & (x=k)
\end{cases}
$$
Then，
$$
\begin{align}
& \frac{\partial L(\theta)}{\partial p(k)}=\frac{c_k}{p(k)}-\lambda=0 \\
& c_1=\lambda \ p(1) \\
& c_2=\lambda \ p(2) \\
& \vdots \\
& c_k=\lambda \ p(k) \\
& \vdots \\
& c_n=\lambda \ p(n) \\
\end{align}
$$
综上所述，
$$
\begin{align}
& \sum_{k=1}^n c_k = \lambda\sum_{k=1}^n p(k) \\
& 由于\ \ \sum_{k=1}^nc_k = N； \sum_{k=1}^n p(k)=1 \\
& 于是\ \ \lambda = N \\
& p(k)=\frac{c_k}{\lambda}=\frac{c_k}{N}
\end{align}
$$

## 2-1 主成分分析（PCA）

将数据映射到低维空间上，轴代表主成分（原始变量的线性组合）

将$\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_N \in R^n$映射到$y_1,y_2,\cdots,y_N \in R$：$y_i=\boldsymbol{w}^T \boldsymbol{x}_i$；$\boldsymbol{w}$：directional vector（$\boldsymbol{w}^T\boldsymbol{w}=\boldsymbol{1}$）

### Mean

$$
\bar{y}=\frac{1}{N}\sum_{i=1}^Ny_i = \frac{1}{N}\sum_{i=1}^Nw^T\boldsymbol{x}_i = \boldsymbol{w}^T\frac{1}{N}\sum_{i=1}^N\boldsymbol{x}_i = \boldsymbol{w}^T \bar{\boldsymbol{x}}
$$

### Variance

$$
\begin{align}
\sigma^2 &= \frac{1}{N}\sum_{i=1}^N(y_i-\bar{y})^2 \\
&= \frac{1}{N}\sum_{i=1}^N (\boldsymbol{w}^T\boldsymbol{x}_i-\boldsymbol{w}^T\bar{\boldsymbol{x}})^2 \\
&= \frac{1}{N}\sum_{i=1}^N (\boldsymbol{w}^T\boldsymbol{x}_i-\boldsymbol{w}^T\bar{\boldsymbol{x}})(\boldsymbol{x}_i^T\boldsymbol{w}-\bar{\boldsymbol{x}}^T\boldsymbol{w}) \\
&= \boldsymbol{w}^T\frac{1}{N}\sum_{i=1}^N(\boldsymbol{x}_i-\bar{\boldsymbol{x}})(\boldsymbol{x}_i-\bar{\boldsymbol{x}})^T\boldsymbol{w} \\
&= \boldsymbol{w}^T S\boldsymbol{w}
\end{align}
$$

其中$S$是原始数据的协方差矩阵

### Optimization Problem

$$
maximize_\boldsymbol{w} \ \ \ \boldsymbol{w}^TS\boldsymbol{w} \\
subject \ \  to \ \  \boldsymbol{w}^T\boldsymbol{w}=1
$$

拉格朗日形式：
$$
L(\boldsymbol{w})=\boldsymbol{w}^TS\boldsymbol{w}-\lambda(\boldsymbol{w}^T\boldsymbol{w}-1)
$$
目标是找到最优的$\boldsymbol{w}$：
$$
\begin{align}
& \frac{\partial L(\boldsymbol{w})}{\partial \boldsymbol{w}}=2\boldsymbol{w}^TS-2\lambda\boldsymbol{w}^T=0 \\
& S\boldsymbol{w}=\lambda\boldsymbol{w}
\end{align}
$$
$\boldsymbol{w}_k$是特征值$\lambda_k$对应的特征向量（$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$）
$$
\begin{align}
\sigma_k^2 &=\boldsymbol{w}_k^TS\boldsymbol{w}_k \\
&= \boldsymbol{w}_k^T \lambda_k \boldsymbol{w}_k \\
&= \lambda_k
\end{align}
$$
所以选择特征向量可以按照对应大的特征值来选

### 第二个主成分

$$
\begin{align}
max_\boldsymbol{w} \ \ \ & \boldsymbol{w}^TS\boldsymbol{w} \\
subject \ \  to \ \  & \boldsymbol{w}^T\boldsymbol{w}=1 \\
& \boldsymbol{w}^T\boldsymbol{w}_1=0
\end{align}
$$

拉格朗日形式：
$$
\begin{align}
& L(\boldsymbol{w})=\boldsymbol{w}^TS\boldsymbol{w}-\alpha_1(\boldsymbol{w}^T\boldsymbol{w}-1)-\alpha_2(\boldsymbol{w}^T\boldsymbol{w}_1) \\
& \frac{\partial L(\boldsymbol{w})}{\partial \boldsymbol{w}}=2\boldsymbol{w}^TS-2\alpha_1\boldsymbol{w}^T-\alpha_2 \boldsymbol{w}_1^T=0 \\
\end{align}
$$
右乘一个$\boldsymbol{w}_1$：
$$
2\boldsymbol{w}^TS\boldsymbol{w}_1-2\alpha_1\boldsymbol{w}^T\boldsymbol{w}_1-\alpha_2 \boldsymbol{w}_1^T\boldsymbol{w}_1=0
$$
由于$S\boldsymbol{w}_1=\lambda_1\boldsymbol{w}_1$；$\boldsymbol{w}^T\boldsymbol{w}_1=0$；$\boldsymbol{w}_1^T\boldsymbol{w}_1=1$，最终得到：
$$
S\boldsymbol{w}=\alpha_1\boldsymbol{w}
$$
这也是一个关于协方差矩阵$S$的特征值问题，因此第二个主成分就可选择第二大的特征值对应的特征向量。

<u>实际上，主成分分析就是选择协方差矩阵的前$m$个特征值及其对应的特征向量</u>，<u>从而将$n$维数据映射到$m$维上</u>

($n \geq m$)     $\boldsymbol{y}=\boldsymbol{W}_m^T\boldsymbol{x}$     $\boldsymbol{W}_m^T \in R^{m \times n}$ 

==The larger the variance, the larger the mean amount of information==

Amount of information loss:
$$
\boldsymbol{x}^T\boldsymbol{x}-\boldsymbol{y}^T\boldsymbol{y} \\
\sum_{k=m+1}^n \lambda_k
$$
方差贡献率：$\frac{\lambda_k}{\sum_{k=1}^n \lambda_k}$；累计方差贡献率：$\frac{\sum_{k=1}^m\lambda_k}{\sum_{k=1}^n \lambda_k}$





## 2-2 非线性主成分分析

用一个非线性函数$\hat{\varphi}(x)$将原始数据映射到一个更高维的空间，然后在高维空间做PCA再降到一个低维空间

Original Data：$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N \in \boldsymbol{R}^n$

Higher-Dimensional Data：$\boldsymbol{z}_1,\boldsymbol{z}_2,\cdots,\boldsymbol{z}_N \in \boldsymbol{R}^l$ ，$\boldsymbol{z}=\hat{\varphi}(\boldsymbol{x})$ 

Principal Component Data：$\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_N \in \boldsymbol{R}^m$，$\boldsymbol{y}=\boldsymbol{W}^T\boldsymbol{z}$

方差：$\sigma^2=\frac{1}{N}\sum_{i=1}^N(y_i-\bar{y})^2$ 
$$
\begin{align}
& y_i=\boldsymbol{W}^T\boldsymbol{z}_i, \ \ \ \bar{y}=\frac{1}{N}\sum_{i=1}^N y_i = \frac{1}{N}\sum_{i=1}^N \boldsymbol{W}^T\boldsymbol{z}_i = \boldsymbol{W}^T \bar{\boldsymbol{z}} \\
& \sigma^2 = \frac{1}{N}\sum_{i=1}^N (\boldsymbol{W}^T\boldsymbol{z}_i - \boldsymbol{W}^T \bar{\boldsymbol{z}})^2
\end{align}
$$
令$\boldsymbol{\varphi(x)}=\boldsymbol{z}-\boldsymbol{\bar{z}}$，方差变为如下：
$$
\begin{align}
\sigma^2 &= \frac{1}{N}\sum_{i=1}^N (\boldsymbol{W}^T\boldsymbol{\varphi(x_i)})^2 \\
&= \frac{1}{N}\sum_{i=1}^N \boldsymbol{W}^T\boldsymbol{\varphi(x_i)}\boldsymbol{\varphi(x_i)}^T\boldsymbol{W}
\end{align}
$$

 ### Optimization Priblem

$$
\begin{align}
& max_\boldsymbol{w} \ \ \ \frac{1}{N}\sum_{i=1}^N \boldsymbol{W}^T\boldsymbol{\varphi(x_i)}\boldsymbol{\varphi(x_i)}^T\boldsymbol{W} \\
& subject \ \ to \ \ \boldsymbol{W}^T\boldsymbol{W}=1
\end{align}
$$

拉格朗日形式：
$$
\begin{align}
& L(\boldsymbol{w})= \frac{1}{N}\sum_{i=1}^N \boldsymbol{W}^T\boldsymbol{\varphi(x_i)}\boldsymbol{\varphi(x_i)}^T\boldsymbol{W} - \lambda (\boldsymbol{W}^T\boldsymbol{W}-1) \\
& \frac{\partial L(\boldsymbol{w})}{\partial \boldsymbol{w}} = \frac{2}{N}\sum_{i=1}^N \boldsymbol{W}^T\boldsymbol{\varphi(x_i)}\boldsymbol{\varphi(x_i)}^T-2\lambda \boldsymbol{W}^T=0 \\
& \frac{1}{N}\sum_{i=1}^N\boldsymbol{\varphi(x_i)}\boldsymbol{\varphi(x_i)}^T\boldsymbol{W}=\lambda \boldsymbol{W} --------(1)
\end{align}
$$
这里的$\boldsymbol{\varphi(x_i)^Tw}$是一个常数项，令$c_i=\frac{1}{N\lambda}\boldsymbol{\varphi(x_i)^Tw}$，上式变为：
$$
\sum_{i=1}^Nc_i\boldsymbol{\varphi(x_i)=w}-------------(2)
$$
令$\boldsymbol{Z}=[\boldsymbol{\varphi(x_1) \ \ \varphi(x_2) \ \ \cdots \varphi(x_N)}]$，（1）（2）变为：
$$
\frac{1}{N}\boldsymbol{ZZ^Tw}=\lambda\boldsymbol{w}-------------(3) \\
\boldsymbol{Zc}= \boldsymbol{w}-----------------(4)
$$
将（4）代入（3）得到：
$$
\frac{1}{N}\boldsymbol{ZZ^TZc}=\lambda\boldsymbol{Zc}
$$
两边同时左乘$\boldsymbol{Z}^T$：
$$
\frac{1}{N}\boldsymbol{Z}^T\boldsymbol{ZZ^TZc}=\lambda\boldsymbol{Z}^T\boldsymbol{Zc}
$$
令$\boldsymbol{K=Z^TZ}$：
$$
\frac{1}{N} \boldsymbol{KKc}=\lambda\boldsymbol{Kc}
$$
这就变成了矩阵$\frac{1}{N}\boldsymbol{K}$的特征值问题

$\lambda_1 \geq\lambda_2 \geq \cdots \geq \lambda_N；\boldsymbol{c}_1,\boldsymbol{c}_2, \cdots, \boldsymbol{c}_N$分别代表$\frac{1}{N}\boldsymbol{K}$的特征值和对应的特征向量

由（4）$\boldsymbol{w}_i=\boldsymbol{Z}\boldsymbol{c}_i$可知，$\boldsymbol{c}_i$应满足如下条件：
$$
\boldsymbol{w}_i^T\boldsymbol{w}_i=\boldsymbol{c}_i^T\boldsymbol{Z}^T\boldsymbol{Z}\boldsymbol{c}_i=\boldsymbol{c}_i^T\boldsymbol{K}\boldsymbol{c}_i=N\lambda \boldsymbol{c}_i^T\boldsymbol{c}_i=1
$$
方差就等于对应的特征值（证明略）

### Kernel

由
$$
\begin{align}
& \boldsymbol{z=\hat{\varphi}(x)} \\
& \boldsymbol{y=W^Tz} \\
& \boldsymbol{z-\bar{z}=\varphi(x)} \\
& \boldsymbol{w_i=Zc_i} \\
& \boldsymbol{\varphi(x)=\hat{\varphi}(x)} - \frac{1}{N}\sum_{j=1}^N \boldsymbol{\hat{\varphi}(x_j)}
\end{align}
$$
得到：
$$
\begin{align}
& \boldsymbol{y}=
\left[
\matrix{
  \boldsymbol{w}_1^T\\
  \vdots \\
  \boldsymbol{w}_m^T
}
\right]
\boldsymbol{z}=
\left[
\matrix{
  \boldsymbol{w}_1^T\\
  \vdots \\
  \boldsymbol{w}_m^T
}
\right]
\boldsymbol{\hat{\varphi}(x)}=
\left[
\matrix{
  \boldsymbol{c_1^TZ^T}\\
  \vdots \\
  \boldsymbol{c_m^TZ^T}
}
\right]
\boldsymbol{\hat{\varphi}(x)} \\
& \boldsymbol{y} =
\left[
\matrix{
  \boldsymbol{c_1^T}\\
  \vdots \\
  \boldsymbol{c_m^T}
}
\right]
\boldsymbol{Z^T\hat{\varphi}(x)} \\
&= 
\left[
\matrix{
  \boldsymbol{c_1^T}\\
  \vdots \\
  \boldsymbol{c_m^T}
}
\right]
\left[
\matrix{
  \boldsymbol{\varphi{(x_1)}^T}\\
  \vdots \\
 \boldsymbol{\varphi{(x_N)}^T}
}
\right]
\boldsymbol{\hat{\varphi}(x)} \\
&= 
\left[
\matrix{
  \boldsymbol{c_1^T}\\
  \vdots \\
  \boldsymbol{c_m^T}
}
\right]
\left[
\matrix{
  \boldsymbol{\varphi{(x_1)^T}\hat{\varphi}(x)}\\
  \vdots \\
 \boldsymbol{\varphi{(x_N)}^T\hat{\varphi}(x)}
}
\right]--------(7)
\end{align}
$$
$\boldsymbol{\varphi(x_i)^T \hat{\varphi}(x)}$可由以下式子计算得到：
$$
\begin{align}
\boldsymbol{\varphi(x_i)^T \hat{\varphi}(x)} &= (\boldsymbol{\hat{\varphi}(x}_i)-\frac{1}{N}\sum_{j=1}^N \boldsymbol{\hat{\varphi}(x_j)})^T \boldsymbol{\hat{\varphi}(x)} \\
&= \boldsymbol{\hat{\varphi}(x_i)}^T \boldsymbol{\hat{\varphi}(x)}- \frac{1}{N}\sum_{j=1}^N \boldsymbol{\hat{\varphi}(x_j)}^T \boldsymbol{\hat{\varphi}(x)}
\end{align}
$$
==$\boldsymbol{\varphi(x_i)^T \hat{\varphi}(x_j)} = k(\boldsymbol{x_i,x_j})$就是一个**kernel**==；

$\boldsymbol{\varphi(x_i)^T \hat{\varphi}(x)} = k(\boldsymbol{x_i,x})-\frac{1}{N}\sum_{j=1}^Nk(\boldsymbol{x_j,x})$ 



令$\boldsymbol{\hat{Z}}=[\boldsymbol{\hat{\varphi}(x_1) \ \ \hat{\varphi}(x_2) \ \ \cdots \hat{\varphi}(x_N)}]$，则：
$$
\begin{align}
& \boldsymbol{\varphi(x)=\hat{\varphi}(x)} - \frac{1}{N} \boldsymbol{\hat{Z1}} 
\end{align}
$$

$$
\begin{align}
\boldsymbol{Z} &= [\boldsymbol{\varphi(x_1) \ \ \varphi(x_2) \ \ \cdots \varphi(x_N)}] \\
&= [\boldsymbol{\hat{\varphi}(x_1) \ \ \hat{\varphi}(x_2) \ \ \cdots \hat{\varphi}(x_N)}] - \frac{1}{N}\boldsymbol{\hat{Z1}[1 \ \ 1 \cdots\ \ 1]} \\
&= \boldsymbol{\hat{Z}}- \frac{1}{N}\boldsymbol{\hat{Z}11^T}
\end{align}
$$

$$
\begin{align}
\boldsymbol{K}=\boldsymbol{Z^TZ} &= (\boldsymbol{\hat{Z}-\frac{1}{N}\hat{Z}11^T})^T (\boldsymbol{\hat{Z}-\frac{1}{N}\hat{Z}11^T}) \\
&= (\boldsymbol{I-\frac{1}{N}11^T})\boldsymbol{\hat{Z}^T\hat{Z}}(\boldsymbol{I-\frac{1}{N}11^T})
\end{align}
$$

$$
\boldsymbol{\hat{K}}=\boldsymbol{\hat{Z}^T\hat{Z}}=
\left[
\matrix{
  \boldsymbol{\hat{\varphi}(x_1)^T}\\
  \vdots \\
 \boldsymbol{\hat{\varphi}(x_N)^T}
}
\right]
[\boldsymbol{\hat{\varphi}(x_1)} \ \ \cdots \ \ \boldsymbol{\hat{\varphi}(x_N)}]
$$

 $\boldsymbol{\hat{Z}^T\hat{Z}}$的第$(i,j)$个元素是$kernel$ $k(\boldsymbol{x}_i,\boldsymbol{x}_j)=\boldsymbol{\hat{\varphi}(x_i)^T\hat{\varphi}(x_j)}$ 



### Algorithm

- 定义$kernel$ $k(\boldsymbol{x}_i,\boldsymbol{x}_j)$
- 计算矩阵$\hat{\boldsymbol{K}}$，其$(i,j)$个第个元素是$k(\boldsymbol{x}_i,\boldsymbol{x}_j)$ 
- 计算$\boldsymbol{K}=(\boldsymbol{I}-\frac{1}{N}\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{\hat{K}}(\boldsymbol{I}-\frac{1}{N}\boldsymbol{1}\boldsymbol{1}^T)$ 
- 由$\frac{1}{N}\boldsymbol{K}$的特征向量计算$\boldsymbol{C}=[\boldsymbol{c_1\cdots \boldsymbol{c_m}}]$ 
- 计算向量$\boldsymbol{k(x)}$，其$i$第个元素是$\boldsymbol{\varphi(x_i)^T \hat{\varphi}(x)} = k(\boldsymbol{x_i,x})-\frac{1}{N}\sum_{j=1}^Nk(\boldsymbol{x_j,x})$ 
- 用$PCA$计算得到低维数据：$\boldsymbol{y}=\boldsymbol{C}^T\boldsymbol{k(x)}$ 

==这样就不用再求原始数据映射到高维空间的数据$z$，而是直接定义一个到高维空间的内积，将原始数据映射到低维空间中得到$y$== 



 




































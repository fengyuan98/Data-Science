## 2-3 判别分析（Discriminant Analysis）

Find a ==low-dimensional subspace== that reduces overlap of data distribution between different classes.

将数据映射到低维空间之后在进行分类

第$j$个属于类$i$的数据：$\boldsymbol{x}_{ij} \in \boldsymbol{R}^n$，$i=1,2,\cdots, K$，$j=1,2,\cdots, N_i$ 

$K$：类的个数

$N_i$：属于第i类的数据的个数

$N$：数据总个数
$$
y=\boldsymbol{w^Tx}
$$
第$i$类数据的均值：
$$
\begin{align}
& \bar{\boldsymbol{x}}_i=\frac{1}{N_i}\sum_{j=1}^{N_i}\boldsymbol{x}_{ij} \\
& \bar{y}_i = \frac{1}{N_i}\sum_{j=1}^{N_i}y_{ij} = \frac{1}{N_i}\sum_{j=1}^{N_i}\boldsymbol{w}^T\boldsymbol{x}_{ij} = \boldsymbol{w}^T\frac{1}{N_i}\sum_{j=1}^{N_i}\boldsymbol{x}_{ij} = \boldsymbol{w}^T\bar{\boldsymbol{x}_{i}}
\end{align}
$$
所有数据的均值：
$$
\begin{align}
& \bar{\boldsymbol{x}} = \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i} \boldsymbol{x}_{ij} \\
& \bar{y}= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}y_{ij} = \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}\boldsymbol{w}^T\boldsymbol{x}_{ij}= \boldsymbol{w}^T\frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}\boldsymbol{x}_{ij} = \boldsymbol{w}^T \bar{\boldsymbol{x}}
\end{align}
$$
$y_{ij}$的方差：
$$
\begin{align}
\sigma^2 &= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}(y_{ij}-\bar{y})^2 \\
&= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}(y_{ij}-\bar{y}_i+\bar{y}_i-\bar{y})^2 \\
&= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i} (y_{ij}-\bar{y}_i)^2 + \frac{2}{N}\sum_{i=1}^K\{(\bar{y}_i-\bar{y}) \sum_{j=1}^{N_i}(y_{ij}-\bar{y}_i)\} + \frac{1}{N}\sum_{i=1}^KN_i(\bar{y}_i -\bar{y})^2 \\
 &= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}(y_{ij}-\bar{y}_i)^2 + \frac{1}{N}\sum_{i=1}^K N_i(\bar{y}_i-\bar{y}) \\
& (由于\sum_{j=1}^{N_i}(y_{ij}-\bar{y}_i)\}=0) \\

\end{align}
$$
类内方差：$S_A=\frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}(y_{ij}-\bar{y}_i)^2$

类间方差：$S_B=\frac{1}{N}\sum_{i=1}^K N_i(\bar{y}_i-\bar{y}) $

总体方差：$\sigma^2=S_A+S_B$ 

目标：找到$\boldsymbol{w}$使得组内方差很小，同时组间方差很大

### Optimization Problem

$$
max_\boldsymbol{w}\ \ \ \frac{S_B}{S_A}
$$

> $$
> \begin{align}
> S_A &= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i}(\boldsymbol{w}^T\boldsymbol{x}_{ij}-\boldsymbol{w}^T\bar{\boldsymbol{x}}_i)^2 \\
> &= \frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i} \boldsymbol{w}^T (\boldsymbol{x}_{ij}-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_{ij}-\bar{\boldsymbol{x}}_i)^T\boldsymbol{w} \\
> &= \boldsymbol{w}^T\frac{1}{N}\sum_{i=1}^K \sum_{j=1}^{N_i} (\boldsymbol{x}_{ij}-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_{ij}-\bar{\boldsymbol{x}}_i)^T\boldsymbol{w} \\
> &= \boldsymbol{w}^T \Sigma_A \boldsymbol{w} \\
> S_B &= \frac{1}{N}\sum_{i=1}^K N_i(\boldsymbol{w}^T\bar{\boldsymbol{x}}_i-\boldsymbol{w}^T\bar{\boldsymbol{x}})^2 \\
> &= \frac{1}{N}\sum_{i=1}^K N_i \boldsymbol{w}^T(\bar{\boldsymbol{x}}_i-\bar{\boldsymbol{x}})(\bar{\boldsymbol{x}}_i-\bar{\boldsymbol{x}})^T\boldsymbol{w} \\
> &= \boldsymbol{w}^T\frac{1}{N}\sum_{i=1}^K N_i (\bar{\boldsymbol{x}}_i-\bar{\boldsymbol{x}})(\bar{\boldsymbol{x}}_i-\bar{\boldsymbol{x}})^T\boldsymbol{w} \\
> &= \boldsymbol{w}^T \Sigma_B \boldsymbol{w} \\
> & (the \ rank \ of \ \Sigma_B \ is \ K-1)
> \end{align}
> $$

于是优化问题变为：
$$
max_\boldsymbol{w} \ \ \frac{\boldsymbol{w}^T\Sigma_B\boldsymbol{w}}{\boldsymbol{w}^T\Sigma_A\boldsymbol{w}}
$$
 如果添加限制条件：$\boldsymbol{w}^T\Sigma_A\boldsymbol{w} =1$
$$
max_\boldsymbol{w} \ \ \boldsymbol{w}^T\Sigma_B\boldsymbol{w}\ \ ; \ subject \ to \ \boldsymbol{w}^T\Sigma_A\boldsymbol{w} =1
$$
其拉格朗日形式为：
$$
\begin{align}
& L(\boldsymbol{w})=\boldsymbol{w}^T\Sigma_B\boldsymbol{w} -\lambda(\boldsymbol{w}^T\Sigma_A\boldsymbol{w} - 1) \\
& \frac{\partial L(\boldsymbol{w})}{\partial \boldsymbol{w}}= 2\boldsymbol{w}^T\Sigma_B-2\lambda \boldsymbol{w}^T\Sigma_A=0 \\
& \Sigma_B \boldsymbol{w} = \lambda \Sigma_A \boldsymbol{w}  \ \ \ \ \ (\Sigma_A^{-1}\Sigma_B \boldsymbol{w}=\lambda\boldsymbol{w}) \\
& \boldsymbol{w}_k^T\Sigma_B\boldsymbol{w}_k = \lambda_k\boldsymbol{w}_k^T\Sigma_A\boldsymbol{w}_k = \lambda_k
\end{align}
$$
第一个$\boldsymbol{w}_1$就是$\lambda_1$对应的特征向量

$P_{12}$之后的第2，3...个轴的选择方法证明已略

==判别分析与主成分的区别：目标函数不同？主成分只考虑数据总体的方差的最大化？== 

## 3-1 典型相关（Canonical correlation analysis）

将两组(向量)数据集分别映射到两个低维空间，使得映射后的数据组之间有较高的相关性

原始数据：
$$
\boldsymbol{x}_1，\boldsymbol{x}_2，\cdots，\boldsymbol{x}_N \in R^n \\
\boldsymbol{y}_1，\boldsymbol{y}_2，\cdots，\boldsymbol{y}_N \in R^m
$$
映射之后：
$$
u=\boldsymbol{a}^T\boldsymbol{x} \\
v=\boldsymbol{b}^T\boldsymbol{y}
$$
假设$\boldsymbol{x}, \ \boldsymbol{y}$都是标准化之后的数据（均值为0，方差为1）

### Optimization Problem

$$
max_{\boldsymbol{a,b}}\ \ \frac{1}{N}\sum_{i=1}^Nu_iv_i \\
subject \ to \ \frac{1}{N}\sum_{i=1}^Nu_i^2=1;\ \ \frac{1}{N}\sum_{i=1}^Nv_i^2=1
$$

> $$
> \frac{1}{N}\sum_{i=1}^Nu_iv_i=\frac{1}{N}\sum_{i=1}^N\boldsymbol{a^Tx_iy_i^Tb}=\boldsymbol{a}^T\frac{1}{N}\sum_{i=1}^N\boldsymbol{x_iy_i^Tb}=\boldsymbol{a^T}\Sigma_{xy}\boldsymbol{b} \\
> $$
>
> $\Sigma_{xy}$ ：$\boldsymbol{x}, \ \boldsymbol{y}$的协方差矩阵
> $$
> \begin{align}
> \frac{1}{N}\sum_{i=1}^Nu_i^2 &= \frac{1}{N}\sum_{i=1}^N\boldsymbol{a^Tx_ix_i^Ta} \\
> &= \boldsymbol{a}^T \frac{1}{N}\sum_{i=1}^N\boldsymbol{x_ix_i^Ta} \\
> &= \boldsymbol{a}^T\Sigma_x\boldsymbol{a} \\
> &= 1
> \end{align}
> $$
>
> $$
> \begin{align}
> \frac{1}{N}\sum_{i=1}^Nv_i^2 &= \frac{1}{N}\sum_{i=1}^N\boldsymbol{b^Ty_iy_i^Tb} \\
> &= \boldsymbol{b}^T \frac{1}{N}\sum_{i=1}^N\boldsymbol{y_iy_i^Tb} \\
> &= \boldsymbol{b}^T\Sigma_y\boldsymbol{b} \\
> &= 1
> \end{align}
> $$

优化问题可重新写为：
$$
\begin{align}
& max_{\boldsymbol{a,b}} \ \ \boldsymbol{a}^T\Sigma_{xy}\boldsymbol{b} \\
& subject \ to\ \ \ \boldsymbol{a}^T\Sigma_{x}\boldsymbol{a}=1\ \ \ \ \ \ \ \boldsymbol{b}^T\Sigma_{y}\boldsymbol{b}=1
\end{align}
$$
其拉格朗日形式：
$$
L(\boldsymbol{a,b})=\boldsymbol{a}^T\Sigma_{xy}\boldsymbol{b}-\frac{1}{2}\lambda(\boldsymbol{a}^T\Sigma_{x}\boldsymbol{a}-1)-\frac{1}{2}\mu(\boldsymbol{b}^T\Sigma_{y}\boldsymbol{b}-1)
$$

$$
\begin{align}
& \frac{\partial L(\boldsymbol{a,b})}{\partial \boldsymbol{a}}=\boldsymbol{b}^T\Sigma_{xy}^T-\lambda\boldsymbol{a}^T\Sigma_x=0 ------(1)\\
& \frac{\partial L(\boldsymbol{a,b})}{\partial \boldsymbol{b}}=\boldsymbol{a}^T\Sigma_{xy}-\mu\boldsymbol{b}^T\Sigma_y=0 ------(2)
\end{align}
$$

化简之后得到：
$$
\Sigma_x^{-1}\Sigma_{xy}\Sigma_y^{-1}\Sigma_{xy}^T\boldsymbol{a}=\mu\lambda\boldsymbol{a}
$$
这是一个$\Sigma_x^{-1}\Sigma_{xy}\Sigma_y^{-1}\Sigma_{xy}^T$矩阵的特征值问题.

### 特征向量的选取

设矩阵$\Sigma_x^{-1}\Sigma_{xy}\Sigma_y^{-1}\Sigma_{xy}^T$的特征值为$\alpha_k=\mu_k\lambda_k$，其对应的特征向量为$\boldsymbol{a}_k$ 

由（1）式可得：
$$
\begin{align}
\boldsymbol{b}^T\Sigma_{xy}^T\boldsymbol{a}_k=\lambda_k\boldsymbol{a}_k^T\Sigma_{x}\boldsymbol{a}_k=\lambda_k \\
\boldsymbol{a}_k^T\Sigma_{xy}\boldsymbol{b}=\lambda_k -----(3)
\end{align}
$$
由（2）式可得：
$$
\begin{align}
\boldsymbol{a}_k^T\Sigma_{xy}\boldsymbol{b} &=\mu_k\boldsymbol{b}^T\Sigma_{y}\boldsymbol{b} \\
\boldsymbol{a}_k^T\Sigma_{xy}\boldsymbol{b}&=\mu_k -----(4)
\end{align}
$$
（3）（4）的左边是目标函数

$\lambda_k=\mu_k$ Is the value of the objective function.

第一个选择 ($\lambda_1=\mu_1=\sqrt{\alpha_1}$)对应的特征向量$\boldsymbol{a}_1$

#### b 的计算

由（2）可得：
$$
\begin{align}
\Sigma_{xy}^T\boldsymbol{a} &= \mu \Sigma_y\boldsymbol{b} \\
\Sigma_y^{-1}\Sigma_{xy}^T\boldsymbol{a} &= \mu\boldsymbol{b} \\
\boldsymbol{b} &= \frac{1}{\mu}\Sigma_y^{-1}\Sigma_{xy}^T\boldsymbol{a}
\end{align}
$$
将$\boldsymbol{x,y}$映射到$k$维空间，使得映射后的数据集相关性高：
$$
\boldsymbol{u}=
 \left[
\matrix{
  \boldsymbol{a}_1^T\\
  \vdots\\
  \boldsymbol{a}_k^T 
}
\right]\boldsymbol{x}=\boldsymbol{A^Tx} \\
\boldsymbol{v}=
 \left[
\matrix{
  \boldsymbol{b}_1^T\\
  \vdots\\
  \boldsymbol{b}_k^T 
}
\right]\boldsymbol{y}=\boldsymbol{B^Ty} \\
$$

## 3-2 回归分析（Regression）

数据集：$\boldsymbol{S}=\{(\boldsymbol{x}_i,y_i)|i=1,2,\cdots,N;\boldsymbol{x}_i \in R^n,y_i \in R \}$ 

### Optimization Problem

$$
min_\boldsymbol{a}\ \ \ J(\boldsymbol{w})=\sum_{i=1}^N(y_i-\boldsymbol{W^Tx_i})^2
$$

$$
\begin{align}
& \frac{\partial J(\boldsymbol{w})}{\partial \boldsymbol{w}}=2\sum_{i=1}^N(y_i-\boldsymbol{w^Tx_i})\boldsymbol{x}_i^T=0 \\
& \sum_{i=1}^Ny_i\boldsymbol{x}_i^T=\sum_{i=1}^N\boldsymbol{w^Tx_i x_i^T}
\end{align}
$$

写成矩阵形式为：
$$
\begin{align}
& \boldsymbol{y^TX^T=w^TXX^T}\\
& \boldsymbol{Xy=XX^Tw} \\
& \boldsymbol{w=(XX^T)^{-1}Xy}
\end{align}
$$
当矩阵$A \in R^{m\times n} (m>n)$且$rank(A)=n$，则其$pseudoinverse matrix$是：
$$
A^\# = (A^TA)^\#A^T = (A^TA)^{-1}A^T
$$
由此
$$
\boldsymbol{w}=(\boldsymbol{X}^T)^\# \boldsymbol{y}
$$

## 3-3 高斯过程回归（Gaussian Process Regression）

表达预测不确定性的范围（虽然总趋势在回归线上，但有的点离回归线较远，分散比较大）

假设：给定一些 X的值，我们对Y建模，并假设对应的这些Y值服从联合正态分布！（均值假设为0，协方差矩阵为K）

　　如果两个x 比较相似（eg, 离得比较近），那么对应的y值的==相关性==也就较高。换言之，协方差矩阵是 X 的函数。（而不是y的函数）

​		上述的相似性的度量就是$Kernel\ \ Functions$ [(symmetric) positive semi-definite matrix]

> 矩阵正定：对任意的$X≠0$恒有$X^TAX>0$ 
>
> 判定A是半正定矩阵的充要条件：A的所有顺序主子式大于或等于0

> Mercer Theorem：一个矩阵是positive semi-definite matrix当且仅当该矩阵是一个Mercer Kernel

数据集：$\boldsymbol{S}=\{(\boldsymbol{x}_i, y_i)|i=1,2,\cdots ,N;\boldsymbol{x}_i \in R^n,y_i \in R  \}$

目标：找到能描述数据的最好的函数$y=\boldsymbol{w^Tx+\varepsilon};$     $\varepsilon \sim N(0,\sigma^2)$ 

Noise：$\varepsilon_i = y_i-\boldsymbol{w^Tx_i}$

** 每个数据组$(\boldsymbol{x}_i, y_i)$是相互独立的

在给定X的条件下Y的概率为：
$$
\begin{align}
p(\boldsymbol{y|X,w})&=\prod_{i=1}^N p(y_i|\boldsymbol{x_i,w}) \\
&= \frac{1}{(2\pi\sigma^2)^{\frac{N}{2}}}exp\{-\frac{1}{2\sigma^2}(\boldsymbol{y-X^Tw})^T(\boldsymbol{y-X^Tw}) \}
\end{align}
$$
则：
$$
p(\boldsymbol{w|X,y})=\frac{p(\boldsymbol{X,y|w})p(\boldsymbol{w})}{p(\boldsymbol{X,y})}=\frac{p(\boldsymbol{y|X,w})p(\boldsymbol{X|w})p(\boldsymbol{w})}{p(\boldsymbol{X,y})}
$$
去掉不影响参数$\boldsymbol{w}$的项（同时X的分布与$\boldsymbol{w}$无关）：
$$
\begin{align}
p(\boldsymbol{w|X,y}) &\propto p(\boldsymbol{y|X,w})p(\boldsymbol{w}) \\
p(\boldsymbol{w|X,y}) &\propto \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}exp\{-\frac{1}{2\sigma^2}(\boldsymbol{y-X^Tw})^T(\boldsymbol{y-X^Tw}) \} \\ &\times \frac{1}{(2\pi|\Sigma_w|)^{\frac{n}{2}}}exp\{-\frac{1}{2}\boldsymbol{w}^T\Sigma_w^{-1}\boldsymbol{w} \} \\
& \propto exp\{-\frac{1}{2\sigma^2}(\boldsymbol{y-X^Tw})^T(\boldsymbol{y-X^Tw})--\frac{1}{2}\boldsymbol{w}^T\Sigma_w^{-1}\boldsymbol{w} \}
\end{align}
$$
==第9个课件的第8页的推导没看懂==

上式的指数部分展开得到：
$$
-\frac{1}{2\sigma^2}\boldsymbol{w^TXX^Tw}-\frac{1}{2}\boldsymbol{w}^T\Sigma_w^{-1}\boldsymbol{w}+\frac{1}{\sigma^2}\boldsymbol{y^TX^Tw} -\frac{1}{2\sigma^2}\boldsymbol{y^Ty} 
$$
于是得到：
$$
p(\boldsymbol{w|X,y}) \propto exp\{-\frac{1}{2}(\boldsymbol{w-\alpha})^T (\frac{1}{\sigma^2}\boldsymbol{XX^T}+\Sigma_w^{-1} )(\boldsymbol{w-\alpha}) \}
$$
Mean vector $\boldsymbol{w}$：$\boldsymbol{\alpha =\frac{1}{\sigma^2}A^{-1}Xy}$

Variance-covariance matrix $\boldsymbol{A^{-1}}$：$\boldsymbol{A^{-1}}=(\frac{1}{\sigma^2}\boldsymbol{XX^T}+\Sigma_w^{-1})^{-1}$

<u>**对于输入$\hat{\boldsymbol{x}}$，其对应的输出$\hat{y}$的均值和方差分别为（基于$\boldsymbol{w}$的正态分布）：**</u>
$$
\begin{align}
& mean: \ \int \boldsymbol{\hat{x}^Tw}\ p(\boldsymbol{w|X,y})d\boldsymbol{w}=\boldsymbol{\hat{x}^T}\int\boldsymbol{w}\ p(\boldsymbol{w|X,y})d\boldsymbol{w}=\boldsymbol{\hat{x}^T\alpha} \\
& variance: \ \int (\boldsymbol{\hat{x}^Tw-\hat{x}^T\alpha})(\boldsymbol{\hat{x}^Tw-\hat{x}^T\alpha})p(\boldsymbol{w|X,y})d\boldsymbol{w} \\
& =\boldsymbol{\hat{x}^T}\int \boldsymbol{(w-\alpha)(w-\alpha)^T}p(\boldsymbol{w|X,y})d\boldsymbol{w}\ \boldsymbol{\hat{x}} \\
&= \boldsymbol{\hat{x}^T}A^{-1}\boldsymbol{\hat{x}}
\end{align}
$$
如果将原始输入数据映射到高维空间： 
$$
\begin{align}
& \boldsymbol{z=\varphi(x)} \in R^m \\
& y= \boldsymbol{\varphi(x)^Tw}
\end{align}
$$
令$\boldsymbol{\Psi(X)}=[\boldsymbol{\varphi(x_1) \ \ \varphi(x_2)\ \cdots\ \varphi(x_N)}]$，于是$\boldsymbol{A}$和$\boldsymbol{\alpha}$可写为：
$$
\begin{align}
& \boldsymbol{A}=\frac{1}{\sigma^2}\boldsymbol{\boldsymbol{\Psi(X)}\boldsymbol{\Psi(X)}^T}+\Sigma_w^{-1} \\
& \boldsymbol{\alpha} =\frac{1}{\sigma^2}\boldsymbol{A^{-1}}\boldsymbol{\Psi(X)y}
\end{align}
$$
则$\hat{y}$的均值和方差分别为：
$$
\begin{align}
& mean : \boldsymbol{\varphi(\hat{x})^T\alpha} \\
& variance:\boldsymbol{\varphi(\hat{x})^TA^{-1}\varphi(\hat{x})}
\end{align}
$$
令 $\boldsymbol{K=\Psi(X)^T}\Sigma_w\boldsymbol{\Psi(X)}$，则：
$$
\begin{align}
& mean : \boldsymbol{\varphi(\hat{x})^T\alpha}=\boldsymbol{\varphi(\hat{x})^T\Sigma_w\Psi(X)(K+\sigma^2I)^{-1}y} \\
& variance:\boldsymbol{\varphi(\hat{x})^TA^{-1}\varphi(\hat{x})} 
= \boldsymbol{\varphi(\hat{x})^T}\Sigma_w\boldsymbol{\varphi(\hat{x})}-\boldsymbol{\varphi(\hat{x})^T}\Sigma_w\boldsymbol{\Psi(X)}\boldsymbol{(K+\sigma^2I)^{-1}}\boldsymbol{\Psi(X)}^T\Sigma_w\boldsymbol{\varphi(\hat{x})}
\end{align}
$$
定义kernel：$k(\boldsymbol{x,x'})=\boldsymbol{\varphi(x)}^T\Sigma_w\boldsymbol{\varphi(x')}$，则有：
$$
\begin{align}
& \boldsymbol{\varphi(\hat{x})^T}\Sigma_w\boldsymbol{\Psi(X)}=[k(\boldsymbol{\hat{x},x_1}) \ \ k(\boldsymbol{\hat{x},x_2})\ \ \cdots \ \ k(\boldsymbol{\hat{x},x_N})] \\
& \boldsymbol{K=\Psi(X)^T}\Sigma_w\boldsymbol{\Psi(X)}=
\left[
\matrix{
  k(\boldsymbol{x_1,x_1}) && \cdots &&  k(\boldsymbol{x_1,x_N})\\
  \vdots && \cdots && \vdots\\
  k(\boldsymbol{x_N,x_1}) && \cdots &&  k(\boldsymbol{x_N,x_N})
}
\right]
\end{align}
$$
所以，只要知道kernel（inner product）的定义，不需要计算$\boldsymbol{\varphi(x)} $就能算出均值和方差。

## 4-2 Particle Filter













































 ## 4-3 Kalman Filter

## 5-1 EM 

观测数据：$\boldsymbol{x_1,x_2, \cdots ,x_N}$

未观测数据：$\boldsymbol{y_1,y_2, \cdots ,y_N}$

完全数据：$\boldsymbol{z_1,z_2, \cdots ,z_N}$；$\boldsymbol{z_k=\{x_k,y_k\}}$

统计模型的参数集：$\boldsymbol{\theta}$



Goal：solve the optimisation problem of finding statistical model parameters that maximise the <u>**probabilities of generating the observed data**</u> $\boldsymbol{x_1,x_2, \cdots ,x_N}$.

### Optimization Problem 

$$
\begin{align}
& max_\boldsymbol{\theta}\ \ \prod_{i=1}^Np(\boldsymbol{x}_i|\boldsymbol{\theta}) \\
& max_\boldsymbol{\theta}\ \ \sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) \\
\end{align}
$$

$$
\begin{align}
\sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) &= \sum_{i=1}^Nlog \ \int p(\boldsymbol{x_i,y}|\boldsymbol{\theta})d\boldsymbol{y} \\
&= \sum_{i=1}^Nlog \ \int q(\boldsymbol{y}|\boldsymbol{x_i}) \frac{p(\boldsymbol{x_i,y}|\boldsymbol{\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})} d\boldsymbol{y} \\
&= \sum_{i=1}^Nlog \ E_{q(\boldsymbol{y}|\boldsymbol{x_i})}[\frac{p(\boldsymbol{x_i,y}|\boldsymbol{\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})}] \\
&= E_q(q(\boldsymbol{y|x}),\boldsymbol{\theta})
\end{align}
$$

> $E_q[r]$：在分布$q$下$r$的期望值

利用$Jensen's$不等式$log E_q[r]\geq E_q[log\ r]$：
$$
E(q(\boldsymbol{y|x}),\boldsymbol{\theta})\geq \sum_{i=1}^NE_{q(\boldsymbol{y|x_i})}[log \frac{p(\boldsymbol{x_i,y}|\boldsymbol{\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})}]=F(q(\boldsymbol{y|x}),\boldsymbol{\theta})
$$
$F(q(\boldsymbol{y|x}),\boldsymbol{\theta})$是生成观测数据的对数似然的下限（lower limit）

### 目标函数与下限之差

$$
\begin{align}
& E(q(\boldsymbol{y|x}),\boldsymbol{\theta})-F(q(\boldsymbol{y|x}),\boldsymbol{\theta}) \\
&= \sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) - \sum_{i=1}^NE_{q(\boldsymbol{y|x_i})}[log \frac{p(\boldsymbol{x_i,y}|\boldsymbol{\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})}] \\
&= \sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) - \sum_{i=1}^NE_{q(\boldsymbol{y|x_i})}[log \frac{p(\boldsymbol{x_i｜\theta})p(\boldsymbol{y|x_i;\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})}] \\
&= \sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) - \sum_{i=1}^Nlog\ p(\boldsymbol{x}_i|\boldsymbol{\theta}) - \sum_{i=1}^NE_{q(\boldsymbol{y|x_i})}[log \frac{p(\boldsymbol{y|x_i;\theta})}{q(\boldsymbol{y}|\boldsymbol{x_i})}] \\
&= \sum_{i=1}^NE_{q(\boldsymbol{y|x_i})}[log \frac{q(\boldsymbol{y}|\boldsymbol{x_i})}{p(\boldsymbol{y|x_i;\theta})}] \\
&= \sum_{i=1}^N KL(q(\boldsymbol{y|x_i})||p(\boldsymbol{y|x_i;\theta})) \\
& (Kullback-Leibler \ divergence \ for \ distributions \ q(\boldsymbol{y|x_i}) \ and \ p(\boldsymbol{y|x_i;\theta}) )
\end{align}
$$

$\boldsymbol{\theta}^{[t]}$代表第$t$次迭代的模型参数：
$$
\begin{align}
\Delta E &= E(q(\boldsymbol{y|x}),\boldsymbol{\theta}) - E(q(\boldsymbol{y|x}),\boldsymbol{\theta}^{[t]}) \\
&= F(q(\boldsymbol{y|x}),\boldsymbol{\theta}) + \sum_{i=1}^N KL(q(\boldsymbol{y|x_i})||p(\boldsymbol{y|x_i;\theta})) \\ & - F(q(\boldsymbol{y|x}),\boldsymbol{\theta}^{[t]})
 - \sum_{i=1}^N KL(q(\boldsymbol{y|x_i})||p(\boldsymbol{y|x_i;\theta}^{[t]}))
\end{align}
$$
令$q(\boldsymbol{y|x})=p(\boldsymbol{y|x_i;\theta}^{[t]})$，我们得到，上式的$第二项-第四项\geq0$，

要使得$\Delta E$非零，从而目标函数$E(q(\boldsymbol{y|x}),\boldsymbol{\theta})$是递增的，我们应该选择$\boldsymbol{\theta}$ that满足：
$$
F(q(\boldsymbol{y|x}),\boldsymbol{\theta}) \ \ is \ maximized
$$
==观测数据的对数似然的下限越大，目标函数就越可能是递增的==

### E-step

利用模型的$\boldsymbol{\theta}^{[t]}$估算隐藏变量$q(\boldsymbol{y|x})$的分布：
$$
q(\boldsymbol{y|x})=p(\boldsymbol{y|x_i;\theta}^{[t]})
$$

### M-step

找到使得对数下限$F(q(\boldsymbol{y|x),\theta)}$最大化的$\boldsymbol{\theta}$：
$$
max_\boldsymbol{\theta} \ \ F(q(\boldsymbol{y|x),\theta)}
$$
重复E-step和M-step，使得目标函数递增。









































## 5-2 混合高斯（Mixed Gaussian）

## 5-3 贝叶斯推断
### 决策树（Decision Tree）

监督学习

分类决策树：描述对实例进行分类的树形结构，包含：

- 结点（node）

  - 根结点：包含样本的全集

  - 内部结点（internal node）：对应特征或属性测试
  - 叶结点（leaf node）：对应类（决策结果）

- 有向边（directed edge）

决策树也可看作

- if-then-else规则

- 给定特征条件下类的条件概率分布（定义在特征空间的一个划分(partition)上）：将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

---

训(练数据集：$D=\{(x_1,y_1),\cdots,(x_N,y_N)\}$

输入实例（特征向量）：$x_i=(x_i^{(1)},\cdots,x_i^{(n)})^T$   n为特征个数

类标记：$y_i\in \{1,2,\cdots,K\}$     $i=1,2,\cdots,N$     $N$为样本容量

学习目标：根据给定的训练数据集构建决策树模型，对实例进行正确的分类

**决策树的生成对应模型的==局部==选择，决策树的剪枝对应模型的==全局==选择**

#### 3个步骤

##### <u>特征选择：信息增益准则（information gain）</u>

对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

- D：训练数据集，｜D｜：其样本个数
- 设有K个类$C_k$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$ 
- 设特征A有n个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征A的取值将D划分为n个子集$D_1,D_2,\cdots,D_n$，$\sum_{i=1}^n|D_i|=|D|$
- $D_{ik}$：子集$D_i$中属于类$C_k$的样本的集合 $D_{ik}=D_i\bigcap C_k$

> **信息增益的算法**
>
> （1）计算数据集D的经验熵$H(D)$:
> $$
> H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2 \frac{C_k}{D}
> $$
> （2）计算特征A对数据集D的经验条件熵$H(D|A)$：
> $$
> H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2 \frac{|D_{ik}|}{|D_i|}
> $$
> （3）计算特征A的信息增益
> $$
> g(D,A)=H(D)-H(D|A)
> $$
> （4）选择信息增益最大的那个特征
>
> $P_{62}$：例5.2 



> - 熵（entropy）：表示随机变量不确定性的度量
>
>   设X是一个取有限个值的离散随机变量，其概率分布为$p(X=x_i)=p_i,\ \ i=1,2,\cdots,n$ 
>
>   则随机变量X的熵定义为$H(X)=-\sum_{i=1}^np_ilogp_i$   若$p_i=0$,则定义$0log0=0$ 
>
>   这时熵的单位分别称作比特（bit）或纳特（nat）
>
>   熵只依赖于X的分布，而与X的取值无关，所以也可将熵记为$H(p)$ 
>
>   熵越大，随机变量的不确定性就越大：$0\leq H(p)\leq logn$ 
>
> - 条件熵（conditional entropy）：X给定条件下Y的条件概率分布的熵对X的数学期望
>
>   $H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$ 
>
> - 当熵和条件熵中的概率有数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵
>
> #### 信息增益
>
> - 表示得知特征X的信息而使得类Y的信息的不确定性减少的程度
>
> - 特征A对训练数据集D的信息增益$g(D,A)$定义为，集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差：
>   $$
>   g(D,A)=H(D)-H(D|A)
>   $$
>   （表示由于特征A而使得对数据集D的分类的不确定性减少的程度）
>
> 一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息（mutual information）
>
> 决策树学习中的信息增益等价于训练数据集中类与特征的互信息
>
> - 信息增益大的特征往往具有更强的分类能力
> - 其大小是相对训练数据集而言的，并没有绝对的意义；在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，使用信息增益比可以对这一问题进行校正。
>
> #### 信息增益比
>
> - 特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D的经验熵$H(D)$之比：
>
> $$
> g_R(D,A)= \frac{g(D,A)}{H(D)}
> $$



##### <u>决策树生成：</u>

选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。

##### <u>决策树剪枝（Pruning）：</u>

剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。

- 往往通过极小化决策树整体的损失（loss）函数或代价（cost）函数来实现（等价于利用正则化的极大似然估计进行模型选择）

  ｜T｜：树T的叶结点个数

   t：树 t 的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个

  $H_t(T)$：叶结点t上的经验熵

  $\alpha \geq 0$：参数
  $$
  \begin{align}
  &损失函数： \ \ C_\alpha (T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
  \\
  &其中经验熵为：\ \ H_t(T)=-\sum_k \frac{N_{tk}}{N_t}log \frac{N_{tk}}{N_t}
  \\
  &令\ C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^K N_{tk}log \frac{N_{tk}}{N_t}
  \\
  &这时\ \ C_\alpha(T)=C(T)+\alpha |T| 
  \end{align}
  $$
  $C(T)$：模型对训练数据的预测误差，即模型与训练数据的拟合程度

  ｜T｜表示模型复杂度

  参数$\alpha$控制两者之间的影响，较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度

- 剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树
  - 子树越大：拟合越好，模型复杂度越高
  - 子树越小：拟合不好，模型复杂度越低

>输入：生成算法产生的整个树T，参数$\alpha$
>
>输出：修剪后的子树$T_\alpha$
>
>（1）计算每个结点的经验熵
>
>（2）递归地从树的叶结点向上回缩
>
>​			A：回缩前；B：回缩后
>
>​			如果$C_\alpha(T_A) \leq C_\alpha(T_B)$，则进行剪枝，即将父结点变为新的叶结点
>
>（3）返回（2），直至不能继续为止，得到损失函数最小的子树$T_\alpha$
>
>***（2）中的判断只需考虑两个树的损失函数的差，其计算可以在局部进行，所以，决策树的剪枝算法可以由一种动态规划的算法实现

#### 

> ##### 动态规划
>
> 动态规划是自底向上，递归树是自顶向下
>
> 动态规划一般都脱离了递归，而由循环迭代完成计算。
>
> - 状态转移方程



#### 3种典型算法

##### ID3（信息增益）$P_{64}$ 例5.3

- 该算法只有树的生成，所以该算法生成的树容易产生过拟合

输入：训练数据集D，特征集A，阈值$\varepsilon$ 

输出：决策树T

> （1）若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T
>
> （2）若$A= \varnothing$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T
>
> （3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$
>
> （4）如果$A_g$的信息增益小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T
>
> （5）否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
>
> （6）对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用（1）～（5），得到子树$T_i$，返回$T_i$ 

##### C4.5（信息增益比）

输入：训练数据集D，特征集A，阈值$\varepsilon$ 

输出：决策树T

>（1）若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T
>
>（2）若$A= \varnothing$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T
>
>（3）否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$
>
>（4）如果$A_g$的信息增益比小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T
>
>（5）否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
>
>（6）对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用（1）～（5），得到子树$T_i$，返回$T_i$ 

##### CART（基尼系数）（classification and regression tree）

- 在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法

- 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支取值“是”，右分枝取值“否”

  等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元熵确定预测的概率分布

（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大

（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准

- **CART生成**：

  递归地构建二叉决策树的过程

  - 回归树的生成

    用平方误差最小化准则：$\sum_{x_i \in R_m}(y_i-f(x_i))^2$ 

  > **最小二乘回归树生成算法**
  >
  > 

  - 分类树的生成

    用基尼指数最小化准则：

  > **CART生成算法**
  >
  > 

  > **基尼指数**（Gini index）
  >
  > 

- **CART剪枝**

  - 剪枝，形成一个子树序列
  - 在剪枝得到的子树序列$T_0,T_1,\cdots,T_n$中通过交叉验证选取最优子树$T_\alpha$ 

  > **CART剪枝算法**



### 随机森林（Random Forest）

- 由决策树构成的集成算法，属于集成学习中的Bagging（Bootstrap AGgregation的简称）方法
- 由很多决策树构成的，不同决策树之间没有关联
- 其基学习器的多样性不仅来自样本扰动，还来自属性扰动，泛化性能进一步提升
- 其使用的“随机型”决策树只需考察一个属性子集，而Bagging是“确定型”，在选择划分属性时要对结点的所有属性进行考察
- 当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。

#### 4个步骤

- 随机抽样训练决策树
- 随机选取属性做结点分裂属性
- 重复步骤2，直到不能再分裂
- 建立大量决策树形成森林

#### 4种实现方法

- scikit-learn
- Spark MLlib
- DolphinDB
- XGBoost

#### 4个应用方向

- 分类
- 回归
- 聚类
- 异常检测




































### 正则化（regularization）

模型选择的典型方法

是结构风险最小化策略的实现

是经验风险加上一个正则化项（regularizer）或惩罚项（penalty term）

一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大

作用是选择经验风险与模型复杂度同时较小的模型

> - 损失函数（loss function）$L(Y,f(X))$:
>
> 模型输出的预测值$f(X)$与真是值Y的差，用损失函数或者代价函数来度量
>
> 
>
> - 风险函数（risk function）也叫期望损失（expected loss）:
>
> $$
> R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)dxdy
> $$
>
> 即**<u>理论上</u>**模型$f(X)$==关于联合分布==$P(X,Y)$的平均意义下的损失
>
> ** 学习目标：选择期望风险最小的模型
>
> 
>
> - 经验风险（empirical risk）：
>
> $$
> R_{emp}(f)=\frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))
> $$
>
> 即模型$f(X)$==关于训练数据集==的平均损失	
>
> ** 根据大数定律，当样本容量$N$趋于无穷大时，经验风险$R_{emp(f)}$趋于期望风险$R_{exp}(f)$ 
>
> 
>
> - 经验风险最小化（ERM）：
>
> 认为经验风险最小的模型是最优的模型（样本容量足够大时）
> $$
> min_{f \in \mathcal{F}}\ \ \ \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
> $$
> （极大似然就是一个例子：当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计
>
>  
>
> 样本容量很小时，经验风险最小化原则会导致过拟合现象
>
> - 结构风险最小化（structure risk minimisation）：
>
>   - 等价于正则化
>   - 在经验风险上加上表示模型复杂度的正则化项或惩罚项
>   - 结构风险小需要经验风险与模型复杂度同时小
>
>   $$
>   结构风险：R_{srm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f) \\
>   min_{f \in \mathcal{F}} \ \ \ \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
>   $$
>
>   ​	其中$J(f)$为模型的复杂度，是定义在假设空间$\mathcal{F}$上的范函.
>
>   - 例：贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation，MAP）
>
>     当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计.

#### 一般形式

$$
min_{f \in \mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
$$

其中，第一项是经验风险，第二项是正则化项，$\lambda \geq 0$为调整两者之间关系的系数

> 奥卡姆剃刀（Occam's razor）原理
>
> 在所有可选择模型中，能很好解释已知数据并且十分简单的才是最好的模型

**<u>从贝叶斯角度看，正则化项对应于模型的先验概率，可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率</u>**



### 交叉验证（cross-validation）

样本数据充足时：

随机将数据集切分成三部分：训练集（training set）验证集（validation set）和测试集（test set），选择对验证集有最小预测误差的模型

样本数据不足时：

#### 简单交叉验证

#### K折交叉验证 

#### 留一交叉验证






















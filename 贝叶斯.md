### 知识准备

- 先验分布$\pi(\theta)$ + 样本信息$\chi$ $\Rightarrow$ 后验分布$\pi(\theta|x)$

- 条件（后验）概率：$p(A|B)=\frac{p(A\bigcap B)}{p(B)}$

- 联合概率：$p(A\bigcap B)$ 或者$p(A, B)$

- 边缘（先验）概率：在联合概率中，把最终结果中不需要的事件通过合并成他们的全概率，而去消去他们（离散求和，连续积分）

- 贝叶斯定理：
  $$
  p(A|B)=\frac{p(B|A)p(A)}{p(B)}
  $$

### 贝叶斯网络

- 一个有向无环图（Directed Acyclic Graph）由代表变量结点及连接这些结点的有向边组成。

  - 结点代表随机变量，有向边代表结点间的互相关系（父结点指向子结点）
  - 条件概率表示关系强度，没有父结点的用先验概率进行信息表达
  - 结点变量可以是任何问题的抽象

  $$
  p(x_1,\cdots, x_K)=p(x_K|x_1,\cdots,x_{K-1})\cdots p(x_2|x_1)p(x_1)
  $$

- 每个点发生的概率相乘

- 三种形式

  - Head-to-Head

    $a \rightarrow c\ ;\ b\rightarrow c$

    $p(a,b,c)=p(a)*p(b)*p(c|a,b)$ 由于$p(c|a,b)=p(a,b,c)/p(a,b)$ 化简将$p(a,b,c)$消除，得到$p(a,b)=p(a)*p(b)$; 因此在c未知的条件下，a，b被阻断（blocked），是独立的，称为head-to-head条件独立

  - Tail-to-Tail

    $c \rightarrow a\ ;\ c\rightarrow b$ 

    - c未知

      $p(a,b,c)=p(c)*p(a|c)*p(b|c)$ 

      此时无法得出$p(a,b)=p(a)*p(b)$，即a与b不独立

    - c已知

      $p(a,b|c)=p(a,b,c)/p(c)$ 然后将$p(a,b,c)=p(c)*p(a|c)*p(b|c)$ 代入，得到

      $p(a,b|c)=p(c)*p(a|c)*p(b|c)/p(c)=p(a|c)*p(b|c)$,

      即c已知时，a，b独立

  - Head-to-Tail

    衍生出马尔可夫链式模型

    $a \rightarrow c\ ;\ c\rightarrow b$ 

    - c未知

      $p(a,b,c)=p(a)*p(c|a)*p(b|c)$，无法推出a，b独立

    - c已知

      $p(a,b|c)=p(a,b,c)/p(c)$, $p(a,c)=p(a)*p(c|a)=p(c)*p(a|c)$

      化简可得到

      $p(a,b|c)=p(a)*p(c|a)*p(b|c)/p(c)=p(a,c)*p(b|c)/p(c)=p(a|c)*p(b|c)$

      此时a，b被阻断（blocked），是独立的

### 朴素贝叶斯 (Naive Bayes)

==与贝叶斯估计（Bayesian estimation）是不同的概念==

#### 分类器

- 假设各个特征之间相互独立

- 输入：随机向量X

  输出：类标记Y

  基本思想：对给定的输入x，通过学习得到的模型计算后验概率分布$P(Y=C_k|X=x)$，将后验概率最大的类作为x的类输出。
  $$
  P(Y=C_k|X=x)=\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}
  $$

- 步骤

  （1）

  从样本学习得到先验分布$P(Y=C_k)$和条件概率分布$P(X=x|Y=C_k)$ 

  $P(X_1=x_1,\cdots,X_n=x_n|Y=C_k)=P(X_1=x_1|Y=C_k)\cdots P(X_n=x_n|Y=C_k)$

  （2）
  $$
  \begin{align}
  C_{result}&=argmaxP(Y=C_k|X=X^{(test)})\\
  &=argmaxP(X=X^{(test)}|Y=C_k)P(Y=C_k)\\
  &=argmaxP(Y=C_k)\prod_{j=1}^nP(X_j=X_j^{(test)}|Y=C_k)
  \end{align}
  $$
  这里由于分母都是$P(X=X^{(test)})$，所以可以忽略。



#### 朴素贝叶斯法的学习与分类

输入空间： $n$维特征向量的集合$\mathcal{X} \subseteq R^n$ 

输出空间：类标记集合$\mathcal{Y}=\{c_1,c_2,\cdots, c_K\}$ 

训练数据集：$T=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_N,y_N)\}$ 由$P(X,Y)$独立同分布产生

- 通过训练数据集学习联合概率分布$P(X,Y)$，即学习：

  - 先验概率分布：$P(Y=c_k),\ \ \ k=1,2,\cdots,K$ 

  - 条件概率分布：$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),\ \ \ k=1,2,\cdots ,K$

- 条件独立性假设：
  $$
  \begin{align}
  P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\
  &=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
  \end{align}
  $$
  用于分类的特征在类确定的条件下都是条件独立的

- 后验概率（由贝叶斯定理得）：
  $$
  P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
  $$
  将条件独立性假设代入得到：
  $$
  P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
  $$
  由于分母对所有的$c_k$都是相同的，所以可忽略不计

**后验概率最大化：等价于期望风险最小化**

假设$x^{(j)}$可取值有$S_j$个，$Y$可取值有$K$个，那么参数个数为$K\prod_{j=1}^nS_j$

#### 朴素贝叶斯法的参数估计

##### 极大似然估计

- 先验概率$P(Y=c_k)$的极大似然：

$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
$$

- 条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然：

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$

$$
j=1,2,\cdots,n\ ;\ \ l=1,2,\dots, S_j\ ;\ \ k=1,2,\cdots,K
$$

其中，第$j$个特征的可能取值的集合为$\{a_{j1},\cdots,a_{jS_j}\}$；$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值；$I$为指使函数

##### 贝叶斯估计

解决的问题：

用极大似然估计可能会出现所要估计的概率值为0的情况，这时会影响到后验概率的计算结果，使分类产生偏差。

- 条件概率的贝叶斯估计：
  $$
  P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
  \ \ ;\ \ \lambda \geq 0
  $$
  当$\lambda=0$时，就是极大似然估计

  当$\lambda=1$时，就是拉普拉斯平滑（Laplace smoothing）

  对任何$l=1,2,\dots, S_j\ ;\ \ k=1,2,\cdots,K$，有：
  $$
  P_\lambda(X^{(j)}=a_{jl}|Y=c_k)>0. \\
  \sum_{l=1}^{S_j}P(X^{(j)}=a_{jl}|Y=c_k)=1
  $$
  这表明条件概率的贝叶斯估计确实是一种概率分布

- 先验概率的贝叶斯估计：
  $$
  P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}
  $$
  































